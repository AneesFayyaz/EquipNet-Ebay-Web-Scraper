{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import requests\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse, parse_qs, urlencode\n",
    "# Initialize undetected Chrome\n",
    "options = uc.ChromeOptions()\n",
    "options.headless = False  # Change to True if you want headless mode\n",
    "driver = uc.Chrome(options=options)\n",
    "\n",
    "# Open the login page\n",
    "driver.get(\"https://www.equipnet.com/login/\")\n",
    "time.sleep(5)  # Allow time for page load\n",
    "\n",
    "# Attempt to detect an active session\n",
    "already_logged_in = False\n",
    "try:\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, \"//a[@href='/myequipnet/account/']\"))\n",
    "    )\n",
    "    print(\"Already logged in! ‚úÖ\")\n",
    "    already_logged_in = True\n",
    "except:\n",
    "    print(\"No active session detected.\")\n",
    "\n",
    "# If not already logged in, load cookies (if available)\n",
    "if not already_logged_in and os.path.exists(\"cookies.pkl\"):\n",
    "    with open(\"cookies.pkl\", \"rb\") as file:\n",
    "        cookies = pickle.load(file)\n",
    "        for cookie in cookies:\n",
    "            driver.add_cookie(cookie)\n",
    "    print(\"Cookies loaded! Refreshing to check login status...\")\n",
    "    driver.refresh()\n",
    "    time.sleep(5)\n",
    "\n",
    "# Check if login is now successful\n",
    "try:\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, \"//a[@href='/myequipnet/account/']\"))\n",
    "    )\n",
    "    print(\"Login successful! ‚úÖ\")\n",
    "    already_logged_in = True\n",
    "except:\n",
    "    print(\"Proceeding with manual login...\")\n",
    "\n",
    "# If still not logged in, enter credentials manually\n",
    "if not already_logged_in:\n",
    "    driver.find_element(By.ID, \"Username\").send_keys(\"goldenstaterecyclers\")\n",
    "    driver.find_element(By.ID, \"Password\").send_keys(\"Speed2cruisecontrol!\")\n",
    "    driver.find_element(By.ID, \"login-submit\").click()\n",
    "\n",
    "    # Wait for login process\n",
    "    time.sleep(10)\n",
    "\n",
    "    # Verify login\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//a[@href='/myequipnet/account/']\"))\n",
    "        )\n",
    "        print(\"Login successful! ‚úÖ\")\n",
    "\n",
    "        # Save cookies for future use\n",
    "        with open(\"cookies.pkl\", \"wb\") as file:\n",
    "            pickle.dump(driver.get_cookies(), file)\n",
    "        print(\"Cookies saved! ü•≥\")\n",
    "\n",
    "    except:\n",
    "        print(\"Login failed! ‚ùå Exiting script.\")\n",
    "        driver.quit()\n",
    "        exit()\n",
    "    # Open the watchlist page\n",
    "    watchurl = \"https://www.equipnet.com/myequipnet/myoffers/auction-watchlist/\"\n",
    "    driver.get(watchurl)\n",
    "    time.sleep(5)  # Allow page to load\n",
    "\n",
    "    # Extract watchlist URLs\n",
    "    watchlist_links = driver.find_elements(By.XPATH, '//a[contains(text(), \"View Details\")]')\n",
    "\n",
    "    # Store and print extracted URLs\n",
    "    watchlist_urls = [link.get_attribute(\"href\") for link in watchlist_links]\n",
    "\n",
    "    if not watchlist_urls:\n",
    "        print(\"No watchlist items found! Exiting...\")\n",
    "        # driver.quit()\n",
    "        # exit()\n",
    "\n",
    "    print(\"Extracted Watchlist URLs:\")\n",
    "    for url in watchlist_urls:\n",
    "        print(url)\n",
    "\n",
    "    # Extract product links from each watchlist page\n",
    "    product_links = []\n",
    "    try:\n",
    "        for url in watchlist_urls:\n",
    "            driver.get(url)\n",
    "            time.sleep(5)  # Wait for the page to load\n",
    "\n",
    "            # Find all divs with class \"row\" (each row represents a product)\n",
    "            row_divs = driver.find_elements(By.CLASS_NAME, \"col-md-12.form-inline\")\n",
    "\n",
    "            for row_div in row_divs:\n",
    "                # Find all <a> tags inside this row\n",
    "                a_tags = row_div.find_elements(By.TAG_NAME, \"a\")\n",
    "\n",
    "                # Extract href attributes\n",
    "                for a in a_tags:\n",
    "                    link = a.get_attribute(\"href\")\n",
    "                    if link and link not in product_links:  # Avoid duplicates\n",
    "                        product_links.append(link)\n",
    "        print(\"Extracted Product Links:\")\n",
    "        for link in product_links:\n",
    "            print(link)\n",
    "    except Exception as e:\n",
    "        print(\"Error extracting product links:\")\n",
    "# Print the extracted product links\n",
    "\n",
    "def scrape_product_data(url):\n",
    "    driver.get(url)\n",
    "    time.sleep(10)  # Wait for the page to load\n",
    "\n",
    "    product_data = {\n",
    "        \"Lot Number\": \"Not found\",\n",
    "        \"title\": \"Not found\",\n",
    "        \"model\": \"Model not found\",\n",
    "        \"Location\": \"Location not found\",\n",
    "        \"Start Date\": \"Start Date not found\",\n",
    "        \"End Date\": \"Not found\",\n",
    "        \"Link\": url\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        lot_number = driver.find_element(By.CSS_SELECTOR, \"h1.list-view-heading span.orange\").text\n",
    "        product_data[\"Lot Number\"] = lot_number.replace(\"Lot \", \"\").split()[0].strip()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        title = driver.find_element(By.CSS_SELECTOR, \"h1.list-view-heading\").text\n",
    "        product_data[\"Title\"] = title.replace(lot_number, \"\").strip()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        model = driver.find_element(By.XPATH, \"//div[contains(text(), 'Model:')]/following-sibling::div[1]\").text\n",
    "        product_data[\"Model\"] = model.strip()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        start_date = driver.find_element(By.XPATH, \"//h6[contains(text(), 'Event Start Date:')]/following-sibling::p/span\").text\n",
    "        product_data[\"Start Date\"] = start_date.strip()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        location = driver.find_element(By.XPATH, \"//h6[contains(text(), 'Location:')]/following-sibling::p\").text\n",
    "        product_data[\"Location\"] = location.strip()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return product_data\n",
    "\n",
    "# Scrape data from each product URL\n",
    "all_product_data = []\n",
    "for url in product_links:\n",
    "    data = scrape_product_data(url)\n",
    "    if data:  # Only append if data was successfully scraped\n",
    "        all_product_data.append(data)\n",
    "\n",
    "# Create a DataFrame\n",
    "df1 = pd.DataFrame(all_product_data, columns=[\"Lot Number\", \"title\", \"model\", \"Location\", \"Start Date\", \"End Date\", \"Link\"])\n",
    "\n",
    "# Save data to CSV file\n",
    "csv_filename = \"product_data.csv\"\n",
    "df1.to_csv(csv_filename, index=False, encoding=\"utf-8\")\n",
    "print(df1)\n",
    "print(f\"Data saved to {csv_filename} successfully! ‚úÖ\")\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "########################################################\n",
    "#################  EBAY CODE BELOW   ###################\n",
    "########################################################\n",
    "########################################################\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--start-maximized\")\n",
    "chrome_options.add_argument(\"--disable-notifications\")  # Disable notifications\n",
    "\n",
    "# Initialize the WebDriver\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "\n",
    "def switch_tab_to_active(driver):\n",
    "    # Get the current URL\n",
    "    current_url = driver.current_url\n",
    "\n",
    "    # Parse the URL\n",
    "    parsed_url = urlparse(current_url)\n",
    "\n",
    "    # Extract the query parameters\n",
    "    query_params = parse_qs(parsed_url.query)\n",
    "\n",
    "    # Check and modify the 'tabName' parameter\n",
    "    if 'tabName' in query_params:\n",
    "        current_tab = query_params['tabName'][0]\n",
    "        if current_tab == 'SOLD':\n",
    "            query_params['tabName'][0] = 'ACTIVE'\n",
    "        elif current_tab == 'ACTIVE':\n",
    "            query_params['tabName'][0] = 'SOLD'\n",
    "        else:\n",
    "            print(f\"Tab name is not 'SOLD' or 'ACTIVE', it's: {current_tab}\")\n",
    "    else:\n",
    "        # If 'tabName' is not present, set it to 'ACTIVE' by default\n",
    "        query_params['tabName'] = ['ACTIVE']\n",
    "\n",
    "    # Reconstruct the URL with modified parameters\n",
    "    new_query = urlencode(query_params, doseq=True)\n",
    "    new_url = f\"{parsed_url.scheme}://{parsed_url.netloc}{parsed_url.path}?{new_query}\"\n",
    "\n",
    "    # Navigate to the new URL\n",
    "    driver.get(new_url)\n",
    "    print(f\"Navigated to: {driver.current_url}\")\n",
    "\n",
    "def fetch_price_and_process(driver, average_listed_prices, average_sold_prices):\n",
    "    switch_tab_to_active(driver)\n",
    "\n",
    "    # Parse the URL\n",
    "    current_url = driver.current_url\n",
    "    parsed_url = urlparse(current_url)\n",
    "\n",
    "    # Extract the query parameters\n",
    "    query_params = parse_qs(parsed_url.query)\n",
    "\n",
    "    # Check and modify the 'tabName' parameter\n",
    "    if 'tabName' in query_params:\n",
    "        current_tab = query_params['tabName'][0]\n",
    "        if current_tab == 'SOLD':\n",
    "            query_params['tabName'][0] = 'ACTIVE'  # Switch tab to 'ACTIVE' if 'SOLD' is present\n",
    "            is_sold = True\n",
    "        elif current_tab == 'ACTIVE':\n",
    "            query_params['tabName'][0] = 'SOLD'  # Switch tab to 'SOLD' if 'ACTIVE' is present\n",
    "            is_sold = False\n",
    "        else:\n",
    "            print(f\"Tab name is not 'SOLD' or 'ACTIVE', it's: {current_tab}\")\n",
    "    else:\n",
    "        # If 'tabName' is not present, set it to 'ACTIVE' by default\n",
    "        query_params['tabName'] = ['ACTIVE']\n",
    "        is_sold = False  # Default to 'ACTIVE'\n",
    "\n",
    "    # Process page and extract prices\n",
    "    time.sleep(5)\n",
    "    current_url = driver.current_url\n",
    "    try:\n",
    "        try:\n",
    "            # Wait for the tourtip to appear\n",
    "            tourtip = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \".tourtip__mask\"))\n",
    "            )\n",
    "            \n",
    "            # Find the close button within the tourtip\n",
    "            close_button = tourtip.find_element(By.CSS_SELECTOR, \".tourtip__close\")\n",
    "            \n",
    "            # Click on the close button\n",
    "            close_button.click()\n",
    "            \n",
    "            print(\"Tourtip closed successfully.\")\n",
    "        except:\n",
    "            pass\n",
    "        time.sleep(3)\n",
    "        html_content = driver.page_source\n",
    "        \n",
    "        # Create a BeautifulSoup object\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Find the .aggregates div\n",
    "        aggregates = soup.find('div', class_='aggregates')\n",
    "        \n",
    "        if aggregates:\n",
    "            # Find the first .metric-value element\n",
    "            metric_value = aggregates.find('div', class_='metric-value')\n",
    "            \n",
    "            if metric_value:\n",
    "                # Extract the text content\n",
    "                avg_price = metric_value.text.strip()\n",
    "                if is_sold:\n",
    "                    average_sold_prices.append(avg_price)\n",
    "                    print(f\"Avg Sold Price: ${avg_price}\")\n",
    "                else:\n",
    "                    average_listed_prices.append(avg_price)\n",
    "                    print(f\"Avg Listed Price: ${avg_price}\")\n",
    "            else:\n",
    "                print(\"Metric value not found.\")\n",
    "                if is_sold:\n",
    "                    average_sold_prices.append('0')\n",
    "                else:\n",
    "                    average_listed_prices.append('0')\n",
    "        else:\n",
    "            print(\"Aggregates div not found.\")\n",
    "            if is_sold:\n",
    "                average_sold_prices.append('0')\n",
    "            else:\n",
    "                average_listed_prices.append('0')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'An error occurred while trying to get the average listing price: {str(e)}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Navigate to eBay research page\n",
    "url = \"https://www.ebay.com/sh/research?marketplace=EBAY-US&tabName=SOLD\"\n",
    "driver.get(url)\n",
    "print('waiting for 15 seconds')\n",
    "time.sleep(60)\n",
    "print('waiting done for 15 seconds')\n",
    "\n",
    "# Wait for email field to be present\n",
    "email_field = WebDriverWait(driver, 10).until(\n",
    "    EC.presence_of_element_located((By.ID, \"userid\"))\n",
    ")\n",
    "\n",
    "# Fill in email address\n",
    "email_field.send_keys(\"zainiawan5@gmail.com\")\n",
    "\n",
    "# Locate Continue button\n",
    "continue_button = WebDriverWait(driver, 10).until(\n",
    "    EC.element_to_be_clickable((By.ID, \"signin-continue-btn\"))\n",
    ")\n",
    "\n",
    "# Click Continue button\n",
    "continue_button.click()\n",
    "time.sleep(30)\n",
    "# Wait for password field to be present\n",
    "password_field = WebDriverWait(driver, 10).until(\n",
    "    EC.presence_of_element_located((By.ID, \"pass\"))\n",
    ")\n",
    "\n",
    "# Fill in password\n",
    "password_field.send_keys(\"254ed228\")\n",
    "\n",
    "# Locate Sign in button\n",
    "sign_in_button = WebDriverWait(driver, 10).until(\n",
    "    EC.element_to_be_clickable((By.ID, \"sgnBt\"))\n",
    ")\n",
    "\n",
    "# Click Sign in button\n",
    "sign_in_button.click()\n",
    "\n",
    "time.sleep(30)\n",
    "try:\n",
    "    try:\n",
    "        # Wait for Skip for now button to be clickable\n",
    "        skip_button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.ID, \"passkeys-cancel-btn\"))\n",
    "        )\n",
    "\n",
    "        # Click Skip for now button\n",
    "        skip_button.click()\n",
    "        time.sleep(30)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # for index, row in df.iterrows():    \n",
    "    #     try:\n",
    "    #         search_input = WebDriverWait(driver, 10).until(\n",
    "    #             EC.presence_of_element_located((By.CSS_SELECTOR, \".search-input-panel__inputBox input\"))\n",
    "    #         )\n",
    "    #         search_input.clear()\n",
    "    #         search_input.send_keys(f\"{row['title']}, {row['model']}\")\n",
    "    #         time.sleep(3)\n",
    "    #         research_button = WebDriverWait(driver, 10).until(\n",
    "    #             EC.element_to_be_clickable((By.CSS_SELECTOR, \".search-input-panel__research-button\"))\n",
    "    #         )\n",
    "    #         driver.execute_script(\"arguments[0].scrollIntoView(true);\", research_button)\n",
    "    #         research_button.click()            \n",
    "            \n",
    "    #     except:\n",
    "    #         print('erro in the sending text try catch')\n",
    "    average_listed_prices=[]\n",
    "    average_sold_prices=[]\n",
    "\n",
    "\n",
    "    for index, row in df1.iterrows():    \n",
    "        try:\n",
    "            # search_input = WebDriverWait(driver, 10).until(\n",
    "            #     EC.presence_of_element_located((By.CSS_SELECTOR, \".search-input-panel__inputBox input\"))\n",
    "            # )\n",
    "            # # Clear the existing text before sending new text\n",
    "            # search_input.clear()\n",
    "            # search_input.send_keys(f\"{row['title']}, {row['model']}\")\n",
    "            \n",
    "            search_input = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \".search-input-panel__inputBox input\"))\n",
    "            )\n",
    "            # Clear the existing text using JavaScript\n",
    "            driver.execute_script(\"arguments[0].value = ''; arguments[0].dispatchEvent(new Event('input', { bubbles: true }));\", search_input)\n",
    "            # Check if 'model' exists, is not empty, and does not contain \"Model not found\"\n",
    "            if 'model' in row and row['model'].strip() and row['model'].strip().lower() != \"model not found\":\n",
    "                # Search by both model and title\n",
    "                search_input.send_keys(f\"{row['model']} {row['title']}\")\n",
    "            else:\n",
    "                # Search only by title\n",
    "                search_input.send_keys(f\"{row['title']}\")      \n",
    "            \n",
    "            \n",
    "            time.sleep(3)\n",
    "            research_button = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, \".search-input-panel__research-button\"))\n",
    "            )\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView(true);\", research_button)\n",
    "            research_button.click()            \n",
    "        except:\n",
    "            print('error in the sending text try catch')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Function to fetch and process the price\n",
    "\n",
    "\n",
    "        for _ in range(2):\n",
    "            fetch_price_and_process(driver, average_listed_prices, average_sold_prices)\n",
    "\n",
    "        # You can check the result\n",
    "        print(\"Avg Listed Prices:\", average_listed_prices)\n",
    "        print(\"Avg Sold Prices:\", average_sold_prices)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        switch_tab_to_active(driver)\n",
    "        time.sleep(3)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "# Close browser after processing\n",
    "driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df2_columns = pd.DataFrame({\n",
    "    'average_listed_price': average_listed_prices,\n",
    "    'average_sold_prices': average_sold_prices\n",
    "})\n",
    "\n",
    "# Concatenate only the last two columns to df\n",
    "df = pd.concat([df1, df2_columns], axis=1)\n",
    "filename = 'product_data.csv'\n",
    "df.to_csv(filename, index=False)\n",
    "print(f\"\\nData saved to {filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
